{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CS 4476/6476 Project 3: Local Feature Matching](https://sites.google.com/view/cs4476-6476-sp2020)\n",
    "\n",
    "This iPython notebook:  \n",
    "(1) Loads and resizes images  \n",
    "(2) Finds interest points in those images                 (you code this)  \n",
    "(3) Describes each interest point with a local feature    (you code this)  \n",
    "(4) Finds matching features                               (you code this)  \n",
    "(5) Visualizes the matches  \n",
    "(6) Evaluates the matches based on ground truth correspondences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from student_feature_matching import match_features, pca, accelerated_matching\n",
    "from student_sift import get_features\n",
    "from student_harris import get_interest_points\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "from proj3_unit_tests.harris_unit_test import (\n",
    "    test_get_gradients,\n",
    "    test_get_gradients2,\n",
    "    test_gaussian_kernel, \n",
    "    test_second_moment, \n",
    "    test_corner_response, \n",
    "    test_get_interest_points,\n",
    "    test_find_single_valid_corner,\n",
    "    verify\n",
    ")\n",
    "from proj3_unit_tests.sift_unit_test import (\n",
    "    test_get_magnitudes_and_orientations,\n",
    "    test_get_feat_vec,\n",
    "    test_get_features, \n",
    ")\n",
    "\n",
    "from proj3_unit_tests.feature_match_test import (\n",
    "    test_compute_dists,\n",
    "    test_feature_matching,\n",
    "    test_pca\n",
    ")\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('../data/Notre Dame/921919841_a30df938f2_o.jpg')\n",
    "image2 = load_image('../data/Notre Dame/4191453057_c86028ce1f_o.jpg')\n",
    "eval_file = '../data/Notre Dame/921919841_a30df938f2_o_to_4191453057_c86028ce1f_o.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('../data/Mount Rushmore/9021235130_7c2acd9554_o.jpg')\n",
    "# image2 = load_image('../data/Mount Rushmore/9318872612_a255c874fb_o.jpg')\n",
    "# eval_file = '../data/Mount Rushmore/9021235130_7c2acd9554_o_to_9318872612_a255c874fb_o.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('../data/Episcopal Gaudi/4386465943_8cf9776378_o.jpg')\n",
    "# image2 = load_image('../data/Episcopal Gaudi/3743214471_1b5bbfda98_o.jpg')\n",
    "# eval_file = '../data/Episcopal Gaudi/4386465943_8cf9776378_o_to_3743214471_1b5bbfda98_o.pkl'\n",
    "\n",
    "                    \n",
    "scale_factor = 0.5\n",
    "image1 = cv2.resize(image1, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "image2 = cv2.resize(image2, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "image1_bw = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)\n",
    "image2_bw = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "n_pts = 1500 # width and height of each local feature, in pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find distinctive points in each image (Szeliski 4.1.1)\n",
    "### These are the results using the ground truth points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = cheat_interest_points(eval_file, scale_factor)\n",
    "# plt.figure(); plt.imshow(image1_bw)\n",
    "# Visualize the interest points\n",
    "c1 = show_interest_points(image1, x1, y1)\n",
    "c2 = show_interest_points(image2, x2, y2)\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.figure(); plt.imshow(c2)\n",
    "print('{:d} corners in image 1, {:d} corners in image 2'.format(len(x1), len(x2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where you implement and observe your results from get interest points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First some unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify each step in the code, this will check if your implementation is correct or not.\n",
    "\n",
    "## Do not modify the constructor of any function (i.e. to take some custom arguments\n",
    "## as input)\n",
    "\n",
    "\n",
    "print(\"Gaussian Kernel:\", verify(test_gaussian_kernel))\n",
    "print('Gradients test 1:', verify(test_get_gradients))\n",
    "print('Gradients test 2:', verify(test_get_gradients2))\n",
    "print('Second Moments:', verify(test_second_moment))\n",
    "print('Corner Response:', verify(test_corner_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now run it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1, R1, _= get_interest_points(image1_bw, n_pts)\n",
    "x2, y2, R2, _= get_interest_points(image2_bw, n_pts)\n",
    "c1 = show_interest_points(image1, x1, y1)\n",
    "c2 = show_interest_points(image2, x2, y2)\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.figure(); plt.imshow(c2)\n",
    "print('{:d} corners in image 1, {:d} corners in image 2'.format(len(x1), len(x2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Interest Points:', verify(test_get_interest_points))\n",
    "print('Interest Points Single Corner:', verify(test_find_single_valid_corner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature vectors at each interest point (Szeliski 4.1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where you will code the functions in student_sift.py. Run the following cell to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Magnitudes and Orientations:\", verify(test_get_magnitudes_and_orientations))\n",
    "print('Feature Vector', verify(test_get_feat_vec))\n",
    "print('All Feature Vectors', verify(test_get_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect features from our image pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_width = 16\n",
    "image1_features = get_features(image1_bw, x1, y1, feature_width)\n",
    "image2_features = get_features(image2_bw, x2, y2, feature_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match features (Szeliski 4.1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete compute_feature_distances() and feature_matching() in student_feature_matching.py. The following cell will test your implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Distances:\", verify(test_compute_dists))\n",
    "print('Matches:', verify(test_feature_matching))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match features for our image pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "matches, confidences = match_features(image1_features, image2_features, x1, y1, x2, y2)\n",
    "end = time.time()\n",
    "unoptimized = end - start\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))\n",
    "print('Time Elapsed: {}'.format(unoptimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "You might want to set 'num_pts_to_visualize' and 'num_pts_to_evaluate' to some constant (e.g. 100) once you start detecting hundreds of interest points, otherwise things might get too cluttered. You could also threshold based on confidence.  \n",
    "  \n",
    "There are two visualization functions below. You can comment out one of both of them if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 100\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.png', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out the function below if you are not testing on the Notre Dame, Episcopal Gaudi, and Mount Rushmore image pairs--this evaluation function will only work for those which have ground truth available.  \n",
    "  \n",
    "You can use `annotate_correspondences/collect_ground_truth_corr.py` to build the ground truth for other image pairs if you want, but it's very tedious. It would be a great service to the class for future years, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 100\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        x1[matches[:num_pts_to_evaluate, 0]], y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        x2[matches[:num_pts_to_evaluate, 1]], y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(); plt.imshow(c)\n",
    "plt.savefig('../results/eval.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bells and Whistles tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA: \", verify(test_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 24\n",
    "start = time.time()\n",
    "reduced_image1_features, reduced_image2_features = pca(image1_features, image2_features, n_components)\n",
    "matches, confidences = match_features(reduced_image1_features, reduced_image2_features, x1, y1, x2, y2)\n",
    "end = time.time()\n",
    "pca_optimized = end - start\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))\n",
    "print('Time Elapsed (PCA): {}'.format(pca_optimized))\n",
    "print('Time Elapsed (no optimization): {}'.format(unoptimized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 100\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.png', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 100\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        x1[matches[:num_pts_to_evaluate, 0]], y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        x2[matches[:num_pts_to_evaluate, 1]], y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(); plt.imshow(c)\n",
    "plt.savefig('../results/eval.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "matches, confidences = accelerated_matching(image1_features, image2_features, x1, y1, x2, y2)\n",
    "end = time.time()\n",
    "accelerated = end - start\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))\n",
    "print('Time Elapsed (Accelerated): {}'.format(accelerated))\n",
    "print('Time Elapsed (PCA): {}'.format(pca_optimized))\n",
    "print('Time Elapsed (no optimization): {}'.format(unoptimized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 100\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.png', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 100\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        x1[matches[:num_pts_to_evaluate, 0]], y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        x2[matches[:num_pts_to_evaluate, 1]], y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(); plt.imshow(c)\n",
    "plt.savefig('../results/eval.png', dpi=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
